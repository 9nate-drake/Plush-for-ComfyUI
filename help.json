{
    "sp_help": "\u2022  This node's name is 'Enhancer' in the double click node 'Search' dialogue for ComfyUI. \n\n\u2022  Use 'Show Text|pysssss' nodes for displaying text output from Plush nodes.  Plush outputs text as UTF-8 Unicode, which Show Text can display correctly.\n\n\n****************\n\n\n\u2726 GPTmodel: Select the ChatGPT model you want to use to generate the prompt.  'gpt-4'1106-preview' is the new turbo gpt-4.\n\n\u2726 creative_latitude:  Higher numbers give the model more freedom to interpret your prompt or image.  Lower numbers constrain the model to stick closely to your input.\n\n\u2726 tokens: A limit on how many tokens are made available for ChatGPT to use, it doesn't have to use them all.\n\n\u2726 style: Choose the art style you want to base your prompt on.  If this list is too long, type a few characters of the style you're looking for and the list will dynamically filter.\n\n\u2726 artist: Will produce a 'style of' phrase listing the number of artists you indicate.  They will be artists that work in the chosen style.  Choose 0 if you don't want this.\n\n\u2726 prompt_style: 'Narrative' is long form grammatically correct creative writing, This is the preferred form for Dall-e. 'Tags' is a terse, stripped down list of visual attributes without grammatical phrasing, This is the preferred form for SD and Midjourney.\n\n\u2726 max_elements: A limit on the number of distinct descriptions of visual elements in the prompt. Smaller numbers makes a shorter prompt.\n\n\u2726 style_info: Set to True if you want background information about the art style you chose.",
    "wrangler_help": "\u2022  Use 'Show Text|pysssss' nodes for displaying text output from Plush nodes.  Plush outputs text as UTF-8 Unicode, which Show Text can display correctly.\n\n\u2022 Exif Wrangler will extract Exif and/or AI generation workflow metadata from .jpg (.jpeg) and .png images.  .jpg photographs can be queried for their camera settings.  ComfyUI's .png files will yield certain values from their workflow including the prompt, seed etc.  Images from other AI generators may or may not yield data depending on where they store their metadata. For instance Auto 1111 .jpg's will yield their workflow information that's stored in their Exif comment.\n\n**************\n  \n\u2726 write_to_file: Whether or not to save the meta data file you see in the output to a .txt file in the: '.../ComfyUI/output/PlushFiles' directory.\n\n\u2726 file_prefix: The prefix for the file name of the saved file, this will be appended to a date/time value to make the file unique. The file will have a .txt extension: e.g., 'MyFileName_ew_20240204_193224.txt'\n\n\u2726 Min_Prompt_len:  A filter value for prompts: Exif Wrangler has to distinguish between actual prompts and other long strings in the ComfyUI embeded meta data.  Every Note, every text display box, and even some text that's hidden in nodes is included in the JSON that holds this information.  This field allows you to set a minimum length for strings to be displayed to help filter out shorter unwanted text strings.\n\n\u2726 Alpha_Char_Pct: Another prompt filter that works by only allowing text strings that have a percentage of alpha ASCII characters (Aa - Zz plus comma) equal to or higher than this setting.  Increasing the percentage screens out strings that have lots of bytes, symbols and numbers.  If you use a lot of weightings or Lora values in your prompts that introduce angle brackets, parentheses, brackets and colons, you may have to lower this percentage to see your prompt.  \n\n\u2726 Prompt_Filter_Term:  Enter a single term or short phrase here. A particular prompt string will only be included in Possible Prompts if it contains an exact match for this term.  This can be used in a couple of ways:  \n 1) If you know there's a term you always or frequently use in the prompts, or if you remember part of a particular image prompt's wording,  you can add it here before you click the Queue button.  \n 2) If, after clicking Queue, a lot of Possible Prompt candidates clutter your output.  Find the one you know is the actual prompt, find a unique word or phrase in it e.g.: 'regal'.  Enter that word or phrase as a filter term and run Wrangler again.  You'll get back an uncluttered response to save as a file.\n\n***************\n\n\u2726 troubleshooting output:  Hook this output up to a text display node to see any INFO/WARNING/ERROR data that's generated during this node's run. ",
	"dalle_help": "\u2022  Use 'Show Text|pysssss' nodes for displaying text output from Plush nodes.  Plush outputs text as UTF-8 Unicode, which Show Text can display correctly.\n\n\u2022 Dall-e Image will produce an image .PNG from a text prompt using the Dall-e 3 model from OpenAI. It requires a OpenAI API key.\n\n**************\n\n\u2726 GPTmodel: The Dall-e model that will generate the image file.  Currently this is limited to Dall-e 3.\n\n\u2726 prompt: The text prompt for the image you want to produce.  Be aware that OpenAI will generate their own prompt from your prompt and pass that to the image model.\n\n\u2726 image_size: Choose a square, portrait or landscape image.  The image size format is: Width, Height.  The 1792 image sizes cost slightly more tokens.\n\n\u2726 image_quality: Self explanatory, you can experiment to see if you think there's a noticable difference.  The standard quality image costs a few less tokens than hd.\n\n\u2726 style: Vivid produces a little more contrast and more saturated colors.  The choice depends on what type of image you're trying to produce.\n\n\u2726  batch_size:  The number of images you want to produce in one run.  The vast majority of the times batches run without incident, but you should be aware that sending image requests to the Dall-e server is not as reliable as running images locally in SD.  If the server gets overtaxed, or hiccups you may not get back all the images you requested. This Dall-e node will handle OpenAI server errors gracefully and allow your batch to continue to completion, but sometimes you may get back fewer images than you requested.  If you keep the 'troubleshooting' output connected it will report any errors and let you know how many images were processed vs how many you requested.\n\n\u2726  seed:  This works just like a seed in a KSampler except that it doesn't affect a latent or the image.  It's simply there for you to set to: 'randomize' or 'increment' if you want Dall-e to run with every Queue, or to 'fixed' if you only want Dall-e to run once per prompt or setting.  The Dall_e API doesn't actually pass seed values.  This can also be controlled by the 'Global Seed' from the Inspire Pack. \n\n***************\n\n\u2726 troubleshooting output:  Hook this output up to a text display node to see any INFO/WARNING/ERROR data that's generated during this node's run.\n\n\u2726  Dalle_e_prompt: The prompt that Dall-e 3 generates from your prompt.  This is the prompt that actually gets passed to the image model.  Hook up a text display node to see it.",
	"adv_prompt_help": "\u2022 Advanced Prompt Enhancer uses Large Language Models to generate text output from any combination of: Instruction, Example(s), Image and Prompt you provide. No OpenAI API key is needed for Open source LLM's.  This node can use various ChatGPT models and GPT-Vision (image and text processing) if you have an OpenAI API key and have stored it in an environment variable (see ReadMe file).  With or without a key it can also connect to various front-end front-end apps that aggregate open-source LLM's e.g. LM Studio, Oobabooga, Koboldcpp, etc.\n\n\u2022 image input: Advanced Prompt Enhancer can send image data (in the form of a b64 image file) to both ChatGTP-vision and to open source vision models.  If you're sending an image to an open source model be sure both the model and the front-end/manager app have vision capabilities and can handle image files.\n\n**************\n\n\u2022  LLM: This indicates the type of LLM you're going to send your data to.  If you're using something other than 'ChatGPT', you'll need to provide a valid URL in the LLM_URL field near the bottom of the node.  If you're using Oobabooga API make sure you read the LLM_URL help below.  'OpenAI compatible http POST' uses a web POST action rather than the OpenAI API Object to communicate with the local server from the LLM front end, typically this requires a 'v1/chat/completions' path.   \n\n\u2022 GPTmodel: This field only applies when the LLM field is set to 'ChatGPT'.  Select the specific OpenAI ChatGPT model you want to use, if you're including an image in your input the node will automatically use the 'gpt-4-vision' model regardless of what you've chosen here.\n\n\u2022 creative_latitude: (Temperature)  This will set how strictly the LLM adheres to common word relationships and how closely it will follow your instruction and prompt.  Setting this value higher allows more creative freedom in interpreting your input and generating its ouptput.\n\n\u2022 tokens:  The maximum number of tokens that the LLM can use in processing your prompt and return text.  This is not the number of tokens  it 'will' use, it's the number available that it 'can' use.\n\n\u2022 seed: This is a pseudo or mock seed, it has no effect on the text generated, and it's not passed to the LLM.  It's used here solely to control when the node will run.  It works the same as a KSampler, set it to 'fixed' if you want the node to run only once each time you change your inputs, set it to random or increment/decrement if you want it run with each Queue.\n\n\u2022 example_delimiter: You can provide multiple examples to the LLM of how you want the generated output to look (writing style, formatting, use of language, etc).  Providing multiple examples for a given instruction is a type of 'Few Shot Learning', which can be effective with some LLM's. This field indicates how the node will distinguish each separate example.  You can choose to separate your examples with a pipe '|' character, two newlines (i.e.: carriage returns) or two colons '::', these are called delimiters.  You can add other newlines or spaces between examples as you need to for readability as long as these delimiters are somewhere between the separate examples (the extra lines and spaces will be stripped out before being sent to the LLM).\n\n\u2022 enhanced_tag_placement: At the bottom of the node is a text field: 'Misc_tags' where you can enter text that you want to appear in the node's generated text output exactly as written, without going through the interpretive process of the LLM.  When enhanced_tag_placement is set to false, this Misc text will appear at the end of the generated text, the default.  When enhanced_tag_placement is set to true, you have the option to mark your text by providing a single asterisk prefix to have it placed at the beginning of the text output, a two asterisks prefix to have it placed in the middle of the text output (immediately after a comma or period), or no asterisks to have it placed in the default position at the end of the output.  If you have more that one text item and this field is set to true, you'll need to separate each text item with a Pipe symbol '|', you can add whatever spaces or newlines for readability as long as there's a pipe between items.  For example if enhanced_tag_ placement was set to true and the Misc_tags field contained: <lora:MyLora:0.4>  |  *High quality Photograph | **(big black cowboy hat:1.6); the lora tag would appear at the end, 'High quality Photograph' would appear at the beginning and '(big black cowboy hat:1.6)' would appear in the middle following a comma or period. Another example: *AI generated caption:   | © 2021 image copyright, Joe Photographer, the 'AI generated' header would appear at the beginning of the text, the copyright notice at the end. \n\n\u2022 The Misc_tags field at the bottom of the node is used when you want to include text in your output exactly as written (e.g. lora tags).  See the enhanced_tag_placement help item for an explanation of how this text is placed in the output.\n\n\u2022 LLM_URL: When using an LLM other than ChatGPT you'll need to provide a URL in this field.  Typically the LLM aggregator application you're using (e.g. LM Studio, Oobabooga), will indicate the URL to use after you startup its server. It may be in the terminal output or in the UI. Some LLM frontends will specify that a particular URL is OpenAI compatible.  If so this is the one you want to use.  Typically these URL have this general format: http://localhost:5001/v1 where '5001' is the port and 'localhost' is interchangable with '127.0.0.1'.  If you're using the Oobabooga API or 'OpenAI compatible http POST' selection your url will need to have /chat/completions appended as part of the url: http://127.0.0.1:5000/v1/chat/completions\n\n**************\n\n\u2022 Use the troubleshooting output if you have issues with model connections, or if you want to see exactly which model was used to produce your output (some ChatGPT model names are actually only pointers to the latest specific model in that category)." 
}